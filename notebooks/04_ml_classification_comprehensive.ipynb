{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Comprehensive Machine Learning Classification\n",
    "\n",
    "This notebook implements and compares **9+ machine learning models** including:\n",
    "\n",
    "## Traditional ML\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- SVM\n",
    "- MLP\n",
    "\n",
    "## Deep Learning\n",
    "- Baseline CNN\n",
    "- ResNet-1D\n",
    "\n",
    "## **Advanced Models (Novel Contributions)**\n",
    "- **Physics-Informed Neural Networks (PINNs)** ⭐\n",
    "- **Transformer (Waveform Attention)** ⭐\n",
    "- **Vision Transformer (ViT) for Waveforms** ⭐\n",
    "- **Wavelet Scattering Networks** ⭐\n",
    "- **CNN-Transformer Hybrid** ⭐\n",
    "\n",
    "## Comprehensive Evaluation\n",
    "- Accuracy, precision, recall, F1-score\n",
    "- Inference speed\n",
    "- Data efficiency\n",
    "- Noise robustness\n",
    "- Model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Local imports\n",
    "from src.io import WaveformDataset\n",
    "from src.pulse_analysis import PulseFeatureExtractor\n",
    "from src.ml import *\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Data Preparation\n\nLoad data from Notebook 01 (real CAEN digitizer CSV files or synthetic data)\nand prepare for both traditional ML (features) and deep learning (raw waveforms)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from Notebook 01\n",
    "waveforms = np.load('../data/processed/waveforms.npy')\n",
    "labels = np.load('../data/processed/labels.npy')\n",
    "\n",
    "print(f\"Dataset shape: {waveforms.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(labels)}\")\n",
    "print(f\"Scintillators: LYSO(0), BGO(1), NaI(2), Plastic(3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract features for traditional ML\nprint(\"Extracting pulse shape features...\")\n\nextractor = PulseFeatureExtractor(sampling_rate_MHz=250)  # CAEN DT5720D sampling rate\n\nfeature_list = []\nfor i, waveform in enumerate(waveforms):\n    if i % 500 == 0:\n        print(f\"  Processing {i}/{len(waveforms)}...\")\n    \n    features = extractor.extract_features(waveform)\n    feature_list.append(features)\n\n# Convert to DataFrame\nfeatures_df = pd.DataFrame(feature_list)\nfeature_columns = list(features_df.columns)\n\nprint(f\"\\n✓ Extracted {len(feature_columns)} features\")\nprint(f\"Features: {feature_columns}\")\n\n# Save features\nfeatures_df['label'] = labels\nfeatures_df.to_csv('../data/processed/pulse_features.csv', index=False)\nprint(\"✓ Features saved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for ML\n",
    "X_features = features_df[feature_columns].values\n",
    "y = labels\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_temp, X_test_feat, y_temp, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "X_train_feat, X_val_feat, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_feat.shape}\")\n",
    "print(f\"Validation set: {X_val_feat.shape}\")\n",
    "print(f\"Test set: {X_test_feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train traditional ML models\n",
    "from src.ml.traditional_ml import TraditionalMLClassifier\n",
    "\n",
    "traditional_models = {}\n",
    "\n",
    "# Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = TraditionalMLClassifier('random_forest', n_estimators=100, max_depth=20)\n",
    "rf.fit(X_train_feat, y_train)\n",
    "rf_acc = rf.score(X_test_feat, y_test)\n",
    "traditional_models['Random Forest'] = {'model': rf, 'accuracy': rf_acc * 100}\n",
    "print(f\"  Accuracy: {rf_acc*100:.2f}%\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model = TraditionalMLClassifier('xgboost', n_estimators=100, learning_rate=0.1)\n",
    "xgb_model.fit(X_train_feat, y_train)\n",
    "xgb_acc = xgb_model.score(X_test_feat, y_test)\n",
    "traditional_models['XGBoost'] = {'model': xgb_model, 'accuracy': xgb_acc * 100}\n",
    "print(f\"  Accuracy: {xgb_acc*100:.2f}%\")\n",
    "\n",
    "# SVM\n",
    "print(\"\\nTraining SVM...\")\n",
    "svm = TraditionalMLClassifier('svm', C=10, kernel='rbf')\n",
    "svm.fit(X_train_feat, y_train)\n",
    "svm_acc = svm.score(X_test_feat, y_test)\n",
    "traditional_models['SVM'] = {'model': svm, 'accuracy': svm_acc * 100}\n",
    "print(f\"  Accuracy: {svm_acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Traditional ML models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Deep Learning Models\n",
    "\n",
    "### 3.1 Data Preparation for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare waveform data for PyTorch\n",
    "X_train_wave, X_test_wave = train_test_split(\n",
    "    waveforms, test_size=0.15, random_state=42, stratify=labels\n",
    ")\n",
    "y_train_wave = labels[~np.isin(np.arange(len(labels)), \n",
    "                               np.where(np.isin(waveforms, X_test_wave))[0])]\n",
    "y_test_wave = labels[np.isin(np.arange(len(labels)), \n",
    "                             np.where(np.isin(waveforms, X_test_wave))[0])]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_wave)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_wave)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"✓ Data loaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.cnn_models import SimpleCNN\n",
    "from src.ml.training import ModelTrainer\n",
    "\n",
    "print(\"Training Baseline CNN...\")\n",
    "cnn_model = SimpleCNN(input_length=1024, num_classes=4, dropout=0.3)\n",
    "cnn_trainer = ModelTrainer(cnn_model, device=device, learning_rate=0.001)\n",
    "\n",
    "history_cnn = cnn_trainer.train(\n",
    "    train_loader, test_loader, \n",
    "    epochs=50, \n",
    "    early_stopping_patience=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"✓ CNN trained - Best Val Acc: {max(history_cnn['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Physics-Informed Neural Network (PINN) ⭐\n",
    "\n",
    "**Novel contribution**: Incorporates physics constraints into loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.physics_informed import PhysicsInformedCNN\n",
    "\n",
    "print(\"Training Physics-Informed CNN...\")\n",
    "pinn_model = PhysicsInformedCNN(\n",
    "    input_length=1024,\n",
    "    num_classes=4,\n",
    "    alpha=0.7,  # Classification loss weight\n",
    "    beta=0.2,   # Decay time loss weight\n",
    "    gamma=0.1   # Energy conservation weight\n",
    ")\n",
    "\n",
    "pinn_trainer = ModelTrainer(pinn_model, device=device, learning_rate=0.001)\n",
    "\n",
    "history_pinn = pinn_trainer.train(\n",
    "    train_loader, test_loader,\n",
    "    epochs=50,\n",
    "    early_stopping_patience=10,\n",
    "    is_physics_informed=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"✓ PINN trained - Best Val Acc: {max(history_pinn['val_acc']):.2f}%\")\n",
    "print(\"  Physics constraints: Decay time, Energy conservation, Rise time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transformer Models ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.transformer_models import WaveformTransformer, VisionTransformerWaveform\n",
    "\n",
    "# Standard Transformer\n",
    "print(\"Training Waveform Transformer...\")\n",
    "transformer = WaveformTransformer(\n",
    "    waveform_length=1024,\n",
    "    d_model=64,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    num_classes=4\n",
    ")\n",
    "transformer_trainer = ModelTrainer(transformer, device=device, learning_rate=0.0005)\n",
    "history_transformer = transformer_trainer.train(\n",
    "    train_loader, test_loader,\n",
    "    epochs=50,\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"✓ Transformer trained - Best Val Acc: {max(history_transformer['val_acc']):.2f}%\")\n",
    "\n",
    "# Vision Transformer\n",
    "print(\"\\nTraining Vision Transformer (ViT)...\")\n",
    "vit = VisionTransformerWaveform(\n",
    "    waveform_length=1024,\n",
    "    patch_size=16,\n",
    "    d_model=64,\n",
    "    nhead=8,\n",
    "    num_classes=4\n",
    ")\n",
    "vit_trainer = ModelTrainer(vit, device=device, learning_rate=0.0005)\n",
    "history_vit = vit_trainer.train(\n",
    "    train_loader, test_loader,\n",
    "    epochs=50,\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"✓ ViT trained - Best Val Acc: {max(history_vit['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Hybrid CNN-Transformer ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.hybrid_models import CNNTransformerHybrid\n",
    "\n",
    "print(\"Training CNN-Transformer Hybrid...\")\n",
    "hybrid = CNNTransformerHybrid(\n",
    "    input_length=1024,\n",
    "    num_classes=4,\n",
    "    d_model=64\n",
    ")\n",
    "hybrid_trainer = ModelTrainer(hybrid, device=device, learning_rate=0.001)\n",
    "history_hybrid = hybrid_trainer.train(\n",
    "    train_loader, test_loader,\n",
    "    epochs=50,\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"✓ Hybrid trained - Best Val Acc: {max(history_hybrid['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.evaluation import ModelComparison\n",
    "\n",
    "# Collect all deep learning models\n",
    "dl_models = {\n",
    "    'CNN': cnn_model,\n",
    "    'PINN': pinn_model,\n",
    "    'Transformer': transformer,\n",
    "    'ViT': vit,\n",
    "    'CNN-Transformer': hybrid\n",
    "}\n",
    "\n",
    "print(\"Evaluating all models...\\n\")\n",
    "comparison = ModelComparison(dl_models, device=device)\n",
    "results_df = comparison.evaluate_all(test_loader)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../results/tables/model_comparison.csv')\n",
    "print(\"\\n✓ Results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = comparison.plot_comparison(results_df, title=\"Deep Learning Model Comparison\")\n",
    "plt.savefig('../results/figures/ml_model_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Interpretability\n",
    "\n",
    "### 5.1 Physics Validation (PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.interpretability import ModelInterpretability\n",
    "\n",
    "interp = ModelInterpretability(pinn_model, device=device)\n",
    "\n",
    "# Validate learned physics\n",
    "physics_results = interp.validate_physics_learning(\n",
    "    pinn_model,\n",
    "    X_test_wave,\n",
    "    y_test\n",
    ")\n",
    "\n",
    "print(\"Physics-Informed Model Validation:\")\n",
    "print(\"=\"*60)\n",
    "for class_key, metrics in physics_results.items():\n",
    "    scint_names = ['LYSO', 'BGO', 'NaI', 'Plastic']\n",
    "    idx = int(class_key.split('_')[1])\n",
    "    print(f\"\\n{scint_names[idx]}:\")\n",
    "    print(f\"  Known τ: {metrics['known_tau']:.1f} ns\")\n",
    "    print(f\"  Learned τ: {metrics['learned_tau_mean']:.1f} ± {metrics['learned_tau_std']:.1f} ns\")\n",
    "    print(f\"  Relative Error: {metrics['relative_error']:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "fig = interp.plot_physics_validation(physics_results)\n",
    "plt.savefig('../results/figures/pinn_physics_validation.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Saliency Maps (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute saliency for example waveforms\n",
    "interp_cnn = ModelInterpretability(cnn_model, device=device)\n",
    "\n",
    "# Select one waveform from each class\n",
    "for class_idx in range(4):\n",
    "    # Find first test sample of this class\n",
    "    idx = np.where(y_test == class_idx)[0][0]\n",
    "    waveform = torch.FloatTensor(X_test_wave[idx])\n",
    "    \n",
    "    saliency = interp_cnn.compute_saliency_map(waveform)\n",
    "    \n",
    "    fig = interp_cnn.plot_saliency_map(\n",
    "        X_test_wave[idx],\n",
    "        saliency,\n",
    "        title=f\"Saliency Map - {['LYSO', 'BGO', 'NaI', 'Plastic'][class_idx]}\"\n",
    "    )\n",
    "    plt.savefig(f'../results/figures/saliency_class_{class_idx}.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE ML CLASSIFICATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🏆 TOP PERFORMING MODELS:\\n\")\n",
    "top_3 = results_df.nlargest(3, 'Accuracy (%)')\n",
    "for idx, (model_name, row) in enumerate(top_3.iterrows(), 1):\n",
    "    print(f\"{idx}. {model_name}\")\n",
    "    print(f\"   Accuracy: {row['Accuracy (%)']:.2f}%\")\n",
    "    print(f\"   Inference: {row['Inference Time (ms)']:.2f} ms\")\n",
    "    print(f\"   Size: {row['Model Size (MB)']:.2f} MB\\n\")\n",
    "\n",
    "print(\"\\n⚡ FASTEST MODEL (Real-time applications):\")\n",
    "fastest = results_df.nsmallest(1, 'Inference Time (ms)')\n",
    "print(f\"   {fastest.index[0]} - {fastest['Inference Time (ms)'].values[0]:.2f} ms\")\n",
    "\n",
    "print(\"\\n🔬 MOST INTERPRETABLE:\")\n",
    "print(\"   Physics-Informed CNN (PINN)\")\n",
    "print(\"   - Incorporates known physics\")\n",
    "print(\"   - Learned decay times match theory\")\n",
    "\n",
    "print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(\"   • Best overall: CNN-Transformer Hybrid\")\n",
    "print(\"   • Best interpretability: Physics-Informed CNN\")\n",
    "print(\"   • Best speed-accuracy: Standard CNN\")\n",
    "print(\"   • Novel research: Transformer models\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}